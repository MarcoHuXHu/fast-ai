# 第二课 一些知识概念

## 深度学习三要素
**无限灵活的功能: Universal Approximation Theorem**  
**万能的参数拟合: 反向传播**  
**速度与设备可行: 高性能GPU**  



## 理论部分
-----

### 多层感知器Multi-Layer Perceptron(MLP)

前馈神经网络的一种, 包含至少三层神经节点(输入层, 输出层, 和至少一层隐藏层(hidden layer)). 除了输入节点外, 每个节点都使用非线性激活函数. MLP利用反向传播来训练. 注意与自然语言处理Natural Language Processing(NLP) 相区别.  
MLP是fully connected, 即其中任意节点, 都与下一层的每一个节点有连接, 权重为w<sub>ij</sub>.  

### 激活函数Activation function 
节点的激活函数确定了给定的输入或输入集下, 节点的输出. 如果激活函数都为线性, 则任意多层神经节点都可以优化成两层(input-output). 故而多层感知器一定是非线性激活函数, 比如sigmoid(S函数). 否则多层感知器会退化到单层模式.  
[各类激活函数优缺点比较](https://en.wikipedia.org/wiki/Activation_function)  

### 前馈神经网络Feedforward neural network
节点间连接无环的人工神经网络, 信息从输入层向输出层单相传播

### 通用近似理论[Universal Approximation Theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) 
一个具有 有限的Neuron(神经元), 单层Hidden Layer 的 前馈(参数从输入层向输出层单向传播) 神经网络, 可以用来近似欧几里德空间有界闭合子集上的连续函数.  
换而言之, 前馈神经网络在拥有相应的参数时, 可以用来模拟各种各样的功能.

### Learning Techniques:  
神经网络利用训练集不断优化自身节点间权重, 最终收敛到误差足够小的状态.  此时神经网络习得目标函数.

### 监督学习Supervised learning:
训练集有标签. 即用来训练的每一个例子, 都是输入数据和输出结果成对出现的. 

### 反向传播Backpropagation & 梯度下降Gradient descent:
**反向传播**: 神经网络的一种学习方式. 将神经网络推算出的结果与正确结果比较, 利用预先设定的误差函数计算出误差值, 然后把误差反过来从输出层往输入层传播, 在此过程中调节权重.  

**梯度下降**: 非线性最优化中, 反向传播调节权重的一种方式. 通过计算误差对于权重变化的导数, 来改变权重使得误差减小. 故而反向传播只适用于激活函数可导情况.  
用梯度下降更新参数有两种方式:  
批梯度下降Batch Gradient Descent, 遍历全部数据集算一次损失函数, 然后算函数对各个参数的梯度, 更新梯度.  
随机梯度下降Stochastic Gradient Descent, 每看一个数据就算一下损失函数，然后求梯度更新参数.  

### 过拟合Overfit:
对比于可获取的数据总量来说, 用过多参数, 使得模型只要足够复杂，就可以可以完美地适应数据.



## 实践部分
-----

### 训练集train set 验证集validation set 测试集test set
一般需要将样本分成独立的三部分训练集(train set)，验证集(validation set)和测试集(test set)。其中训练集用来训练模型或确定模型参数，验证集用来确定网络结构或者控制模型复杂程度的参数，而测试集是为了测试已经训练好的模型性能如何。  
换而言之, 训练集和验证集都用于完善模型阶段, 当使用测试集时, 模型已经建立完全, 不应再有任何改动了

养成一个好的习惯: Train和Test的数据分开, Train完成前不去看Test的数据, 保持真实性

### 微调finetune
以课程中dogs&cats问题中vgg16为例, 默认情况下, 直接让vgg16对图像进行预测, 会得到按照vgg16默认的1000个分类进行预测并得到概率, 而要利用vgg16来解决dogs&cats, 则需要finetune, 使得vgg16能给出dog和cat两个分类的概率.

为了更好利用vgg16预测得到的信息(比如图片中有"骨头棒", 按1000个分类预测除了各类猫狗之外, 骨头的概率也较高, 那么结果更有可能是狗而不是猫), 可以在vgg16最后一层之后, 加入一层线性层. 输入为1000, 代表原来默认的1000个分类; 输出为2, 表示dog和cat. 然后将数据代入进行训练和验证.

而vgg16本身最后一层就是一个线性层, 用来生产1000个分类的概率, 所以vgg16的finetune就是pop最后一层, 然后加入新的线性层.

### One-Hot-Encoding
又称为一位有效编码, 方法是使用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候，其中只有一位有效。

比如一个类型有0, 1, 2三种状态, One-Hot-Encoding之后表示为001, 010, 101.


### Batch
梯度下降分为批梯度下降(BGD)和随机梯度下降(SGD), 前者计算量开销大, 计算速度慢, 不支持在线学习; 后者速度比较快, 但是收敛性能不太好, 可能在最优点附近晃来晃去, hit不到最优点, 也有可能两次参数的更新互相抵消掉，造成目标函数震荡的比较剧烈.

为了克服两种方法的缺点, 现在一般采用的是一种折中手段，mini-batch gradient decent, 小批的梯度下降, 这种方法把数据分为若干个批, 按批来更新参数, 这样, 一个批中的一组数据共同决定了本次梯度的方向, 下降起来就不容易跑偏, 减少了随机性, 另一方面因为批的样本数与整个数据集相比小了很多, 计算量也不是很大.