# 第四课

## 优化器Optimizer
在执行SGD的时候, loss的结果对于每个参数的导数如何去求?
Analytical Derivative(分析导数)


back propagation是逐层优化参数还是同时优化?
困在局部最优中, 应该在维度越高(参数越多)越难以出现吧



### Gradient - Momentum
不同于直接利用梯度对参数进行优化, momentum是先对之前所有的梯度取平均值, 然后在利用这个平均值对参数进行优化.  
单就梯度进行优化(最速下降), 搜索路径呈锯齿形, 损失函数很可能陷入到鞍点(saddle point), 即梯度在某些轴上变得很小, 但却未达到最优值.  



### 学习率Learning Rate
Learning Rate决定了参数在梯度下降时随导数变化的大小. 一般SGD会使用一个常数作为lr, 然而这会有两个问题:

1. lr的大小导致的问题
如果lr太小, 则优化太慢, 可能无法在合理的时间内优化出模型; lr太大, 则改变虽然快, 但是会使得模型不稳定, 训练时有可能跳出最优区域, 并使得损失函数loss越来越大; 因此我们需要在训练过程中调整lr.  

2. 所有参数使用同一个lr的问题
而当参数的量级不同时(比如线性拟合中, 斜率=3, 轴距=300), 使用同一个lr就不合适了. 即使对输入数据进行正则化也没用. 这时就需要在每个方向上使用不同的lr, 即每个参数都有一个lr.

动态学习率即综合这两点, 在许多优化器中得到应有, 比如:

#### Adagrad

#### RMSProp

#### Adam

像Adam, RMSProp这些是设定初始lr. (估计第三课中Adam设置lr=0.1会导致失败就是lr太大导致无法优化). 训练模型的时候会先设置一个较大的lr, 在逐渐调小lr, 使得模型更加精确(Learning rate annealing)
