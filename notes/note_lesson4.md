# 第四课

## 优化器Optimizer
在执行SGD的时候, loss的结果对于每个参数的导数如何去求?
Analytical Derivative(分析导数)


back propagation是逐层优化参数还是同时优化?
困在局部最优中, 应该在维度越高(参数越多)越难以出现吧



### Gradient - Momentum
不同于直接利用梯度对参数进行优化, momentum是先对之前所有的梯度取平均值, 然后在利用这个平均值对参数进行优化.  
单就梯度进行优化(最速下降), 搜索路径呈锯齿形, 损失函数很可能陷入到鞍点(saddle point), 即梯度在某些轴上变得很小, 但却未达到最优值.  



### 学习率Learning Rate
Learning Rate决定了参数在梯度下降时随导数变化的大小. 一般SGD会使用一个常数作为lr, 然而这会有两个问题:

1. lr的大小导致的问题
如果lr太小, 则优化太慢, 可能无法在合理的时间内优化出模型; lr太大, 则改变虽然快, 但是会使得模型不稳定, 训练时有可能跳出最优区域, 并使得损失函数loss越来越大; 因此我们需要在训练过程中调整lr. 

2. 所有参数使用同一个lr的问题
而当参数的量级不同时(比如线性拟合中, 斜率=3, 轴距=300), 使用同一个lr就不合适了. 即使对输入数据进行正则化也没用. 这时就需要在每个方向上使用不同的lr, 即每个参数都有一个lr.

动态学习率即综合这两点, 在许多优化器中得到应有, 比如:

#### Adagrad
Adagrad基于参数方向之前的梯度来调整lr, 梯度大则lr小, 反之亦然. 具体为: 对之前的梯度值求平方和再开方(记作: lr2-norm), 下一步的lr为当前lr除以lr2-norm.
```
lr(n+1) = lr(n) / sqrt(sum(d(i) * d(i)))        d(i)代表第i步的梯度
```
由于lr2-norm单调递增, 即lr(n+1)/lr(n)单调递减. 可以看出Adagrad是基于这样一个假设: 损失函数在某个参数方向上调整的路径越长, 在该方向越接近最优值, lr变化越小. 当遇到loss比较平坦这样的情况可能lr就会越来越小然后消耗大量时间. 而同时维护所有参数每步的梯度则开销很大.

#### RMSProp
RMSProp是对Adagrad的改进, 在对梯度求平方和的时候会乘以一个类似于exp(n)的权重, 这样越早期的梯度对更新lr的影响越小. 从而运行损失函数在最优点附近移动而不至于跳出去. 而由于指数权重的作用, 即使梯度已经很小了, 对于lr的改变不会太小, 从而避免在loss比较平坦的地方移动太慢.

#### Adam
Adam则是在RMSProp的基础上加入了gradient momentum, 即gradient的平均值. lr在优化的适合先乘以先前梯度的平均值(这样就考虑到了符号/方向), 然后再除以RMSProp算法中的加权平方和.

#### Eve 

像Adam, RMSProp这些是设定初始lr. (估计第三课中Adam设置lr=0.1会导致失败就是lr太大导致无法优化). 训练模型的时候会先设置一个较大的lr, 在逐渐调小lr, 使得模型更加精确(Learning rate annealing)

