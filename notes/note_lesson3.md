# 第三课

## 概念

### Correlation & Convolution

Correlation:
$$
F \circ I(x) = \sum_{i=1}^{n}F(i)I(x+i)
$$
Convolution:
$$
F \circ I(x) = \sum_{i=1}^{n}F(i)I(x-i)
$$
Convolution就是将filter先转置在做Correlation

以一个3*3的filter为例, 现有一个矩阵A, 其中一部分为:  
$
\left[
  \begin{matrix}
... & A_{i-1,j-1}    &  A_{i-1,j}  &   A_{i-1,j+1} & ...  \\
... & A_{ i,j-1}      & A_{ i,j}     &   A_{ i,j+1} &   ...  \\
... & A_{ i+1,j-1} & A_{ i+1,j} &   A_{ i+1,j+1} & ...
  \end{matrix}
\right]
$

filter矩阵X为:

$
\left[
  \begin{matrix}
   1 & 2 & 3 \\
   4 & 5 & 6 \\
   7 & 8 & 9
  \end{matrix}
\right]
$

则Correlation为:
$
X^{'}_{i,j} = 
1 * A_{i-1,j-1} + 2 * A_{i-1,j} + 3 * A_{i-1,j+1} + 
4 * A_{ i,j-1} + 5 * A_{ i,j} + 6 * A_{ i,j+1} + 
7 * A_{ i+1,j-1} + 8 * A_{ i+1,j} + 9 * A_{ i+1,j+1}
$
Convolution为:
$
X^{'}_{i,j} = 
9 * A_{i-1,j-1} + 8 * A_{i-1,j} + 7 * A_{i-1,j+1} + 
6 * A_{ i,j-1} + 5 * A_{ i,j} + 4 * A_{ i,j+1} + 
3 * A_{ i+1,j-1} + 2 * A_{ i+1,j} + 1 * A_{ i+1,j+1}
$


## Keras中的相关层
这里给出Keras文档中CNN相关的层的作用和部分参数说明

### 常用层
-----
### Dense全联通层
```
keras.layers.core.Dense(units, activation=None, use_bias=True)
```
(省略了初始化, 正则项, 约束项)
比如:
```
model.add(Dense(32, input_shape=(16,)))
```
Dense就是常用的全连接层, 所实现的运算是:  
output = activation(dot(input, kernel)+bias). 即对输入的张量与权重矩阵kernel做点乘, 然后与bias张量相加, 最后在经过非线性激活函数activation.  

其中activation是逐元素计算的激活函数，kernel是本层的权值矩阵，bias为偏置向量，只有当use_bias=True才会添加.



### Dropout
```keras.layers.core.Dropout(rate)```
Dropout将在训练过程中每次更新参数时按一定概率（rate）随机断开输入神经元(Activation=0), 防止过拟合

值得注意到是, 当我们设置了Dropout, 比如rate=0.25(25%), 则训练中每次会有25%的机会被断开, 而测试的时候却不会Dropout, 这样平均下来, 训练中计算出来的权重只有测试时的75%. 因此需要对权重进行处理(好在Keras会在加入Dropout时自动处理的, 不需要手动rescaling了).



### MaxPooling池化层:
**池化Pooling**: 把卷积特征划分成为m\*n个x\*y大小的不相交区域, 计算区域上特征的最大值(MaxPooling), 或者平均值, 作为Pooling后的卷积特征. Pooling后的卷积特征矩阵变成m*n的形状.  

比如在vgg16中, 利用```model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))```来进行Pooling, 这样下一层的特征矩阵就会在x, y方向都变为原来的一半. (strides: 步长值, 若小于pool_size则Pooling区域会相交).  

为了不让信息在Pooling的过程中损失太多, 通常会在Pooling后加入更多的filter. 如vgg中的filter数量在Pooling之后翻倍(64, 128, 256).


------
### Convolution卷积层
与Dense层不同, Convolution层进行的不是矩阵点乘, 而是特征矩阵与filter做卷积.

首先, 利用ZeroPadding给图像边界填充0, 方便卷积.  
然后, 把filter对图像做卷积, 突出特征(比如MNIST例子中, Top filter会突出顶边).  
接着, 对突出了特征的矩阵做Pooling, 一来可以选择区域中最有特点的部分(MaxPooling), 二来可以减小特征矩阵. 从而减少运算量, 并防止过拟合.  
最后, 利用反向传播和梯度下降, 来优化filter.  

训练时, 全联通层通过反向传播调整矩阵中的数值, 于是优化了参数, 而卷积层则是通过反向传播调整filter. 所以初始模型的时候随机选择filter, 然后让模型在训练中自动得到最好的filter(匹配图像与标签正确率高).



### ZeroPadding2D:
对2D输入（如图片）的边界填充0，以控制卷积以后特征图的大小



## 其他
-----

### Softmax
$$
P(x_{j}) = \frac{e^{x_{j}}}{\sum_{i=1}^{n}e^{x_i}}
$$
Activation的一种, 用于最后一层, 对Output进行修饰, 使得所有概率和为1, 且概率最高的接近1, 其余概率较小的接近0, 以接近于One-Hot编码的标签. 而且对于Output为负数的情况, 比使用绝对值更加合理.



### Data Augmentation
当数据不足时, 通过对训练数据的图片进行平移, 旋转, 翻转等操作(shuffle=True)产生更多训练集数据, 而验证数据不应该被改变



### Batch Normalization
Normalization(正则化?标准化), 是把给定数据的每个元素, 减去数据集的平均值, 再除以数据集的标准差. 这样可以使得所有的数据都被缩放到相同的尺度(以"从均值到标准差的距离"为单位). (通常对于图像输入的预处理, 只需要减去平均值而不需要除以标准差.)

这样做的原因是, 一个相较于其他输入数据而言特别大的输入值可能会导致神经网络变得非常不稳定, 这种不平衡的数据可能会叠加到各个层, 使得梯度也非常不平衡, 从而难以优化(比如出现指数爆炸).

然而一般的正则化后还是会有可能出现不平衡的值. 如果只是对激活函数的结果进行正则化, 则仍然无法完全防止SGD产生新的不平衡的值, 而且SGD也会倾向于去抵消这个正则化的优化. 而批正则化在此基础上加入了两个补充:
1. 在对激活函数结果正则化之后, 对结果乘以一个任意设定的参数, 在加上另一个参数, 这样可以设置一个新的标准差和平均值.
2. 把原先正则化的标准差和平均值, 以及新加入到这两个参数都设置成可训练模式(Trainable)
因为正则化被引入到了梯度计算中了, 所以在更新参数的时候也会带有正则化的优化, 这样可以保证权重值不会被训练到过高或过低, 并且保证了如果某一层的确需要调整标准差和平均值的时候可以自动完成.

任何一个神经网络都应该要加入批正则化处理, 能大大加快训练速度(防止不平衡值导致不稳定而难以训练), 而且因为降低了无关的不平衡值的影响, 批正则化处理也有助于降低过拟合.



## 实践

### 更深层次微调

vgg是根据ImageNet的数据来训练模型的, 具有足够的能力来分辨猫和狗(事实上可以分辨猫和狗的种类), 因此只需要修改最后一层的全联通层, 使生成的结果适应问题要求即可. 而对于Statefarm(辨别分心驾驶), 就可能不太管用(难以识别图片中人物的动作含义). 所以要重新训练更多的全联通层. 因为filter仍然可以用于此问题, 因此不需要重新训练卷积层. 但对于医学图像(更高分辨/单色/识别模式不同于一般物体), 则需要重新训练卷积层.

所以如果不需要修改卷积层, 可以先计算卷积后的结果然后保存下来(利用bcolz), 以此为输入, 再加入新的层进行训练, 可以大大节约时间(毕竟卷积计算和反向传播的速度远远慢于全联通层)

### 做mnist的一点体会

mnist是keras里面就有的一个数据集, 直接导入就可以下载并以numpy_array储存. 目的是识别手写数字(28*28像素, 单色)

1. 要记得在输入的数据中加入一个维度, 这是因为卷积层会把HGB通道作为一个维度, 而mnist是单色的, 所以要加入这一个维度, 且大小为1. (RGB则为3)

2. Batch Normalization中等axis, 默认状态下为-1, 对于图像识别, 要设置为表示色彩通道的axis.